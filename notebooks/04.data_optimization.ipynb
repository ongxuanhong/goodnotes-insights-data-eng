{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    DateType,\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://27fb42ea487c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data-optimization-2024-12-19 08:46:39.704452</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=data-optimization-2024-12-19 08:46:39.704452>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"data-optimization-{}\".format(datetime.today()))\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, spark):\n",
    "        self.bucket_name = f\"s3a://warehouse\"\n",
    "        self.spark = spark\n",
    "\n",
    "        # Define storage paths\n",
    "        self.bronze_path = f\"{self.bucket_name}/bronze\"\n",
    "        self.silver_path = f\"{self.bucket_name}/silver\"\n",
    "        self.gold_path = f\"{self.bucket_name}/gold\"\n",
    "\n",
    "    def define_schemas(self):\n",
    "        \"\"\"Define schemas for the datasets\"\"\"\n",
    "        self.interactions_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"timestamp\", TimestampType(), False),\n",
    "                StructField(\"action_type\", StringType(), False),\n",
    "                StructField(\"page_id\", StringType(), False),\n",
    "                StructField(\"duration_ms\", LongType(), False),\n",
    "                StructField(\"app_version\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.metadata_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"join_date\", DateType(), False),\n",
    "                StructField(\"country\", StringType(), False),\n",
    "                StructField(\"device_type\", StringType(), False),\n",
    "                StructField(\"subscription_type\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def ingest_to_bronze(self, csv_path: str, dataset_type: str):\n",
    "        \"\"\"Ingest CSV files to bronze layer in parquet format\"\"\"\n",
    "        schema = (\n",
    "            self.interactions_schema\n",
    "            if dataset_type == \"interactions\"\n",
    "            else self.metadata_schema\n",
    "        )\n",
    "\n",
    "        df = self.spark.read.schema(schema).csv(csv_path)\n",
    "\n",
    "        if dataset_type == \"interactions\":\n",
    "            # Partition by date for interactions\n",
    "            df = df.withColumn(\"partition_date\", F.to_date(\"timestamp\"))\n",
    "            output_path = f\"{self.bronze_path}/interactions\"\n",
    "            partition_by = [\"partition_date\"]\n",
    "        else:\n",
    "            # Partition by country for metadata\n",
    "            output_path = f\"{self.bronze_path}/metadata\"\n",
    "            partition_by = [\"country\"]\n",
    "\n",
    "        df.write.mode(\"append\").partitionBy(partition_by).parquet(output_path)\n",
    "\n",
    "    def process_silver_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Process bronze data into silver layer with cleaned and validated data.\n",
    "        Supports incremental processing by date.\n",
    "\n",
    "        Args:\n",
    "            process_date: Optional date to process. If None, processes current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Read only the partition we need from bronze interactions\n",
    "        interactions_df = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.bronze_path}/interactions\"\n",
    "        ).parquet(f\"{self.bronze_path}/interactions/partition_date={process_date}\")\n",
    "\n",
    "        # For metadata, check if we need to process updates\n",
    "        metadata_path = f\"{self.silver_path}/dim_users\"\n",
    "        metadata_df = self.spark.read.parquet(f\"{self.bronze_path}/metadata\")\n",
    "\n",
    "        # Get existing metadata last modified date if exists\n",
    "        try:\n",
    "            existing_metadata = self.spark.read.parquet(metadata_path)\n",
    "            last_modified = existing_metadata.agg(F.max(\"_modified_date\")).collect()[0][\n",
    "                0\n",
    "            ]\n",
    "        except:\n",
    "            last_modified = None\n",
    "            existing_metadata = None\n",
    "\n",
    "        # Process interactions incrementally\n",
    "        clean_interactions = interactions_df.filter(\n",
    "            F.col(\"duration_ms\").between(0, 7200000)\n",
    "        ).dropDuplicates([\"user_id\", \"timestamp\", \"action_type\", \"page_id\"])\n",
    "\n",
    "        fact_interactions = clean_interactions.select(\n",
    "            \"user_id\",\n",
    "            \"timestamp\",\n",
    "            \"action_type\",\n",
    "            \"page_id\",\n",
    "            \"duration_ms\",\n",
    "            \"partition_date\",\n",
    "        ).withColumn(\"_modified_date\", F.current_date())\n",
    "\n",
    "        # Write fact table incrementally\n",
    "        fact_path = f\"{self.silver_path}/fact_interactions\"\n",
    "\n",
    "        (\n",
    "            fact_interactions.write.mode(\"append\")  # Use append mode for incremental\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\n",
    "                \"replaceWhere\", f\"partition_date = '{process_date}'\"\n",
    "            )  # Overwrite only this partition\n",
    "            .parquet(fact_path)\n",
    "        )\n",
    "\n",
    "        # Process metadata changes\n",
    "        if existing_metadata is not None:\n",
    "            # Identify new or updated metadata records\n",
    "            metadata_df = metadata_df.withColumn(\n",
    "                \"_modified_date\", F.current_date()\n",
    "            ).join(\n",
    "                existing_metadata, \"user_id\", \"left_anti\"\n",
    "            )  # Get only new records\n",
    "        else:\n",
    "            metadata_df = metadata_df.withColumn(\"_modified_date\", F.current_date())\n",
    "\n",
    "        if metadata_df.count() > 0:  # Only process if we have changes\n",
    "            dim_users = metadata_df.dropDuplicates([\"user_id\"]).select(\n",
    "                \"user_id\",\n",
    "                \"join_date\",\n",
    "                \"country\",\n",
    "                \"device_type\",\n",
    "                \"subscription_type\",\n",
    "                \"_modified_date\",\n",
    "            )\n",
    "\n",
    "            # Write dimension table\n",
    "            # For small dimension tables, we can use overwrite mode\n",
    "            # For larger ones, consider using merge/upsert operations\n",
    "            if existing_metadata is None:\n",
    "                write_mode = \"overwrite\"\n",
    "            else:\n",
    "                write_mode = \"append\"\n",
    "\n",
    "            (\n",
    "                dim_users.write.mode(write_mode)\n",
    "                .partitionBy(\"country\")\n",
    "                .parquet(metadata_path)\n",
    "            )\n",
    "\n",
    "        # Return metrics about processed data\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"interactions_processed\": clean_interactions.count(),\n",
    "            \"metadata_updates\": metadata_df.count() if metadata_df.count() > 0 else 0,\n",
    "        }\n",
    "\n",
    "    def process_date_range(self, start_date: datetime, end_date: datetime):\n",
    "        \"\"\"Process a range of dates incrementally\"\"\"\n",
    "        current_date = start_date\n",
    "        processing_metrics = []\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            try:\n",
    "                metrics = self.process_silver_layer(current_date)\n",
    "                processing_metrics.append(metrics)\n",
    "                current_date += timedelta(days=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing date {current_date}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        return processing_metrics\n",
    "\n",
    "    def cleanup_old_partitions(self, retention_days: int = 90):\n",
    "        \"\"\"Clean up old partitions based on retention policy\"\"\"\n",
    "        cutoff_date = datetime.now().date() - timedelta(days=retention_days)\n",
    "\n",
    "        # List partitions\n",
    "        bronze_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.bronze_path}/interactions\"\n",
    "        )\n",
    "        silver_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.silver_path}/fact_interactions\"\n",
    "        )\n",
    "\n",
    "        # Delete old partitions\n",
    "        fs = bronze_partitions.getFileSystem(self.spark._jsc.hadoopConfiguration())\n",
    "\n",
    "        for path in [bronze_partitions, silver_partitions]:\n",
    "            if fs.exists(path):\n",
    "                for partition in fs.listStatus(path):\n",
    "                    partition_date = datetime.strptime(\n",
    "                        partition.getPath().getName().split(\"=\")[1], \"%Y-%m-%d\"\n",
    "                    ).date()\n",
    "\n",
    "                    if partition_date < cutoff_date:\n",
    "                        fs.delete(partition.getPath(), True)\n",
    "\n",
    "    def _calculate_session_metrics(\n",
    "        self,\n",
    "        fact_interactions: DataFrame,\n",
    "        process_date: datetime,\n",
    "        lookback_days: int = 1,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate session-based metrics with window functions, handling session boundaries.\n",
    "\n",
    "        Args:\n",
    "            fact_interactions: DataFrame of interactions\n",
    "            process_date: Date to process\n",
    "            lookback_days: Number of days to look back for ongoing sessions\n",
    "        \"\"\"\n",
    "        # Calculate date range for session boundary handling\n",
    "        start_date = process_date - timedelta(days=lookback_days)\n",
    "        end_date = process_date + timedelta(days=1)  # Include full day\n",
    "\n",
    "        # Create window specs without range specification for lag/lead\n",
    "        user_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        # Create window spec for cumulative operations\n",
    "        cumulative_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        sessions_df = (\n",
    "            fact_interactions.filter(\n",
    "                F.col(\"partition_date\").between(start_date, process_date)\n",
    "            )\n",
    "            .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(user_window))\n",
    "            .withColumn(\n",
    "                \"time_diff_minutes\",\n",
    "                F.when(\n",
    "                    F.col(\"prev_timestamp\").isNotNull(),\n",
    "                    (F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\"))\n",
    "                    / 60,\n",
    "                ).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"is_new_session\",\n",
    "                F.when(F.col(\"time_diff_minutes\") >= 30, 1).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_id\",\n",
    "                F.concat(\n",
    "                    F.col(\"user_id\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.date_format(\"partition_date\", \"yyyyMMdd\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.sum(\"is_new_session\").over(cumulative_window),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate metrics only for sessions that end on process_date\n",
    "        return (\n",
    "            sessions_df.withColumn(\n",
    "                \"next_timestamp\", F.lead(\"timestamp\").over(user_window)\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_end\",\n",
    "                F.when(\n",
    "                    F.col(\"next_timestamp\").isNull()\n",
    "                    | (\n",
    "                        (\n",
    "                            F.unix_timestamp(\"next_timestamp\")\n",
    "                            - F.unix_timestamp(\"timestamp\")\n",
    "                        )\n",
    "                        / 60\n",
    "                        >= 30\n",
    "                    ),\n",
    "                    True,\n",
    "                ).otherwise(False),\n",
    "            )\n",
    "            .filter(\n",
    "                (F.col(\"partition_date\") == process_date)\n",
    "                | (F.col(\"session_end\") == True)\n",
    "            )\n",
    "            .groupBy(\"session_id\")\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"actions_per_session\"),\n",
    "                F.sum(\"duration_ms\").alias(\"session_duration_ms\"),\n",
    "                F.first(\"partition_date\").alias(\"session_date\"),\n",
    "                F.last(\"timestamp\").alias(\"session_end_time\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def create_gold_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Create gold layer with pre-aggregated data and business metrics incrementally.\n",
    "\n",
    "        Args:\n",
    "            process_date: Date to process, defaults to current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Load relevant data from silver layer\n",
    "        fact_interactions = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.silver_path}/fact_interactions\"\n",
    "        ).parquet(f\"{self.silver_path}/fact_interactions/partition_date={process_date}\")\n",
    "\n",
    "        dim_users = self.spark.read.parquet(f\"{self.silver_path}/dim_users\")\n",
    "        broadcast_users = F.broadcast(dim_users)\n",
    "\n",
    "        # Calculate daily metrics\n",
    "        daily_metrics = fact_interactions.groupBy(\"partition_date\").agg(\n",
    "            F.countDistinct(\"user_id\").alias(\"daily_active_users\"),\n",
    "            F.count(\"*\").alias(\"total_actions\"),\n",
    "            F.avg(\"duration_ms\").alias(\"avg_duration_ms\"),\n",
    "        )\n",
    "\n",
    "        # Update monthly metrics\n",
    "        month_start = process_date.replace(day=1)\n",
    "        month_end = (process_date + timedelta(days=32)).replace(day=1) - timedelta(\n",
    "            days=1\n",
    "        )\n",
    "\n",
    "        # Read existing monthly metrics for current month if exists\n",
    "        monthly_path = f\"{self.gold_path}/monthly_metrics\"\n",
    "        try:\n",
    "            existing_monthly = self.spark.read.option(\"basePath\", monthly_path).parquet(\n",
    "                f\"{monthly_path}/month_date={month_start}\"\n",
    "            )\n",
    "        except:\n",
    "            existing_monthly = None\n",
    "\n",
    "        # Calculate monthly metrics for current month\n",
    "        month_interactions = (\n",
    "            self.spark.read.option(\"basePath\", f\"{self.silver_path}/fact_interactions\")\n",
    "            .parquet(f\"{self.silver_path}/fact_interactions\")\n",
    "            .filter(F.col(\"partition_date\").between(month_start, month_end))\n",
    "        )\n",
    "\n",
    "        monthly_metrics = (\n",
    "            month_interactions.withColumn(\n",
    "                \"month_date\", F.date_trunc(\"month\", F.col(\"partition_date\"))\n",
    "            )\n",
    "            .groupBy(\"month_date\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"user_id\").alias(\"monthly_active_users\"),\n",
    "                F.count(\"*\").alias(\"total_monthly_actions\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate session metrics with lookback\n",
    "        session_metrics = self._calculate_session_metrics(\n",
    "            fact_interactions, process_date, lookback_days=1\n",
    "        )\n",
    "\n",
    "        # Write metrics to gold layer\n",
    "        # Daily metrics - append mode with partition replacement\n",
    "        (\n",
    "            daily_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\"replaceWhere\", f\"partition_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/daily_metrics\")\n",
    "        )\n",
    "\n",
    "        # Monthly metrics - replace partition for current month\n",
    "        (\n",
    "            monthly_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"month_date\")\n",
    "            .option(\"replaceWhere\", f\"month_date = '{month_start}'\")\n",
    "            .parquet(monthly_path)\n",
    "        )\n",
    "\n",
    "        # Session metrics - append mode with date partitioning\n",
    "        (\n",
    "            session_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"session_date\")\n",
    "            .option(\"replaceWhere\", f\"session_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/session_metrics\")\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"daily_metrics_updated\": daily_metrics.count(),\n",
    "            \"monthly_metrics_updated\": monthly_metrics.count(),\n",
    "            \"sessions_processed\": session_metrics.count(),\n",
    "        }\n",
    "\n",
    "    def backfill_gold_metrics(\n",
    "        self, start_date: datetime, end_date: datetime, parallel: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backfill gold metrics for a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start date for backfill\n",
    "            end_date: End date for backfill\n",
    "            parallel: Whether to process dates in parallel\n",
    "        \"\"\"\n",
    "        if parallel:\n",
    "            # Create list of dates to process\n",
    "            dates = [\n",
    "                (start_date + timedelta(days=x)).date()\n",
    "                for x in range((end_date - start_date).days + 1)\n",
    "            ]\n",
    "\n",
    "            # Process dates in parallel using Spark\n",
    "            date_df = self.spark.createDataFrame(\n",
    "                [(date,) for date in dates], [\"process_date\"]\n",
    "            )\n",
    "\n",
    "            date_df.repartition(min(len(dates), 50)).foreach(\n",
    "                lambda row: self.create_gold_layer(row.process_date)\n",
    "            )\n",
    "        else:\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                try:\n",
    "                    self.create_gold_layer(current_date)\n",
    "                    current_date += timedelta(days=1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {current_date}: {str(e)}\")\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ETL pipeline\n",
    "etl = ETLPipeline(spark)\n",
    "etl.define_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze layer: Ingest and convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze layer: Ingest and convert to parquet\n",
    "etl.ingest_to_bronze(\n",
    "    \"s3a://warehouse/data/user_interactions_sample.csv\", \"interactions\"\n",
    ")\n",
    "etl.ingest_to_bronze(\"s3a://warehouse/data/user_metadata_sample.csv\", \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: Clean data and create fact/dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 1),\n",
       " 'interactions_processed': 2731,\n",
       " 'metadata_updates': 100001}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Silver layer: Clean data and create fact/dimension tables\n",
    "processing_date = datetime(2023, 1, 1).date()\n",
    "etl.process_silver_layer(processing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: process multiple dates incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date_processed': datetime.date(2023, 1, 2),\n",
       "  'interactions_processed': 2742,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 3),\n",
       "  'interactions_processed': 2696,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 4),\n",
       "  'interactions_processed': 2795,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 5),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 6),\n",
       "  'interactions_processed': 2809,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 7),\n",
       "  'interactions_processed': 2648,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 8),\n",
       "  'interactions_processed': 2771,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 9),\n",
       "  'interactions_processed': 2800,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 10),\n",
       "  'interactions_processed': 2780,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 11),\n",
       "  'interactions_processed': 2827,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 12),\n",
       "  'interactions_processed': 2737,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 13),\n",
       "  'interactions_processed': 2876,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 14),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 15),\n",
       "  'interactions_processed': 2748,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 16),\n",
       "  'interactions_processed': 2793,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 17),\n",
       "  'interactions_processed': 2694,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 18),\n",
       "  'interactions_processed': 2774,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 19),\n",
       "  'interactions_processed': 2673,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 20),\n",
       "  'interactions_processed': 2720,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 21),\n",
       "  'interactions_processed': 2776,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 22),\n",
       "  'interactions_processed': 2779,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 23),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 24),\n",
       "  'interactions_processed': 2790,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 25),\n",
       "  'interactions_processed': 2751,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 26),\n",
       "  'interactions_processed': 2794,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 27),\n",
       "  'interactions_processed': 2717,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 28),\n",
       "  'interactions_processed': 2698,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 29),\n",
       "  'interactions_processed': 2867,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 30),\n",
       "  'interactions_processed': 2721,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 31),\n",
       "  'interactions_processed': 2764,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 2, 1),\n",
       "  'interactions_processed': 2792,\n",
       "  'metadata_updates': 0}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime(2023, 1, 2).date()\n",
    "end_date = datetime(2023, 2, 1).date()\n",
    "etl.process_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- action_type: string (nullable = true)\n",
      " |-- page_id: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- _modified_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int = spark.read.parquet(\n",
    "    \"s3a://warehouse/silver/fact_interactions/partition_date=2023-01-03\"\n",
    ")\n",
    "df_fact_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_int.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "|user_id|          timestamp|action_type|page_id|duration_ms|_modified_date|\n",
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "|u238572|2023-01-03 12:21:00|     delete|p276793|     186484|    2024-12-19|\n",
      "|u884418|2023-01-03 13:55:46|     create|p700863|      39622|    2024-12-19|\n",
      "|u794909|2023-01-03 12:20:18|     create|p056524|     175622|    2024-12-19|\n",
      "|u810403|2023-01-03 01:42:47|     delete|p122892|      48056|    2024-12-19|\n",
      "|u710809|2023-01-03 14:29:37|  page_view|p061245|     138137|    2024-12-19|\n",
      "|u338115|2023-01-03 00:31:22|  page_view|p542897|       7572|    2024-12-19|\n",
      "|u057569|2023-01-03 20:56:24|  page_view|p343492|      68929|    2024-12-19|\n",
      "|u321147|2023-01-03 01:40:16|     create|p402539|     159944|    2024-12-19|\n",
      "|u166863|2023-01-03 21:00:37|     delete|p062736|      37096|    2024-12-19|\n",
      "|u169704|2023-01-03 19:02:39|      share|p992803|      13156|    2024-12-19|\n",
      "|u661358|2023-01-03 21:45:07|  page_view|p400678|      31557|    2024-12-19|\n",
      "|u068642|2023-01-03 18:18:39|     delete|p390334|      49610|    2024-12-19|\n",
      "|u005577|2023-01-03 17:36:26|  page_view|p650594|     162007|    2024-12-19|\n",
      "|u873454|2023-01-03 08:46:11|     create|p590398|     233784|    2024-12-19|\n",
      "|u937734|2023-01-03 03:15:58|      share|p832491|      68942|    2024-12-19|\n",
      "|u587321|2023-01-03 14:12:14|     delete|p508786|      29242|    2024-12-19|\n",
      "|u590783|2023-01-03 22:50:46|     create|p532097|     166304|    2024-12-19|\n",
      "|u860187|2023-01-03 09:02:15|       edit|p714403|     141700|    2024-12-19|\n",
      "|u364781|2023-01-03 18:59:18|  page_view|p575974|      97051|    2024-12-19|\n",
      "|u279924|2023-01-03 13:21:44|       edit|p079435|      33608|    2024-12-19|\n",
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold layer: Create pre-aggregated business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 15),\n",
       " 'daily_metrics_updated': 1,\n",
       " 'monthly_metrics_updated': 1,\n",
       " 'sessions_processed': 2746}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process single date\n",
    "processing_date = datetime(2023, 1, 15).date()\n",
    "metrics = etl.create_gold_layer(processing_date)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- actions_per_session: long (nullable = true)\n",
      " |-- session_duration_ms: long (nullable = true)\n",
      " |-- session_end_time: timestamp (nullable = true)\n",
      " |-- session_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics = spark.read.parquet(\"s3a://warehouse/gold/session_metrics/\")\n",
    "df_sess_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|        session_id|actions_per_session|session_duration_ms|   session_end_time|session_date|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|u150209_20230115_0|                  1|             235478|2023-01-15 11:39:30|  2023-01-15|\n",
      "|u212067_20230115_0|                  1|              82707|2023-01-15 06:16:18|  2023-01-15|\n",
      "|u294155_20230115_0|                  1|              12907|2023-01-15 21:46:11|  2023-01-15|\n",
      "|u296373_20230115_0|                  1|             222310|2023-01-15 08:49:20|  2023-01-15|\n",
      "|u308654_20230115_0|                  1|              69602|2023-01-15 12:10:26|  2023-01-15|\n",
      "|u384261_20230115_0|                  1|             270021|2023-01-15 13:40:07|  2023-01-15|\n",
      "|u399815_20230115_0|                  1|             290451|2023-01-15 10:47:36|  2023-01-15|\n",
      "|u482058_20230115_0|                  1|              28435|2023-01-15 10:34:54|  2023-01-15|\n",
      "|u518961_20230115_0|                  1|             168388|2023-01-15 22:56:30|  2023-01-15|\n",
      "|u528951_20230115_0|                  1|             266728|2023-01-15 07:36:18|  2023-01-15|\n",
      "|u609858_20230115_0|                  1|             270067|2023-01-15 15:59:02|  2023-01-15|\n",
      "|u616504_20230115_0|                  1|               4966|2023-01-15 03:03:13|  2023-01-15|\n",
      "|u753440_20230115_0|                  1|              83393|2023-01-15 16:32:13|  2023-01-15|\n",
      "|u820171_20230115_0|                  1|             166387|2023-01-15 19:00:48|  2023-01-15|\n",
      "|u823530_20230115_0|                  1|              96688|2023-01-15 15:16:51|  2023-01-15|\n",
      "|u045962_20230115_0|                  1|              91880|2023-01-15 17:44:12|  2023-01-15|\n",
      "|u049227_20230115_0|                  1|             214162|2023-01-15 18:06:09|  2023-01-15|\n",
      "|u101625_20230115_0|                  1|             189085|2023-01-15 01:01:31|  2023-01-15|\n",
      "|u178589_20230115_0|                  1|             181083|2023-01-15 00:02:39|  2023-01-15|\n",
      "|u196122_20230115_0|                  1|             164425|2023-01-15 00:40:22|  2023-01-15|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
