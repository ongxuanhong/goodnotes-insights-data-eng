{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    DateType,\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4638e1de1119:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data-optimization-2024-12-20 01:38:37.305615</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://spark-master:7077 appName=data-optimization-2024-12-20 01:38:37.305615>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"data-optimization-{}\".format(datetime.today()))\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "sqlContext = SQLContext(spark)\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, spark):\n",
    "        self.bucket_name = f\"s3a://warehouse\"\n",
    "        self.spark = spark\n",
    "\n",
    "        # Define storage paths\n",
    "        self.bronze_path = f\"{self.bucket_name}/bronze\"\n",
    "        self.silver_path = f\"{self.bucket_name}/silver\"\n",
    "        self.gold_path = f\"{self.bucket_name}/gold\"\n",
    "\n",
    "    def define_schemas(self):\n",
    "        \"\"\"Define schemas for the datasets\"\"\"\n",
    "        self.interactions_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"timestamp\", TimestampType(), False),\n",
    "                StructField(\"action_type\", StringType(), False),\n",
    "                StructField(\"page_id\", StringType(), False),\n",
    "                StructField(\"duration_ms\", LongType(), False),\n",
    "                StructField(\"app_version\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.metadata_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"join_date\", DateType(), False),\n",
    "                StructField(\"country\", StringType(), False),\n",
    "                StructField(\"device_type\", StringType(), False),\n",
    "                StructField(\"subscription_type\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def ingest_to_bronze(self, csv_path: str, dataset_type: str):\n",
    "        \"\"\"Ingest CSV files to bronze layer in parquet format\"\"\"\n",
    "        schema = (\n",
    "            self.interactions_schema\n",
    "            if dataset_type == \"interactions\"\n",
    "            else self.metadata_schema\n",
    "        )\n",
    "\n",
    "        df = self.spark.read.schema(schema).csv(csv_path)\n",
    "\n",
    "        if dataset_type == \"interactions\":\n",
    "            # Partition by date for interactions\n",
    "            df = df.withColumn(\"partition_date\", F.to_date(\"timestamp\"))\n",
    "            output_path = f\"{self.bronze_path}/interactions\"\n",
    "            partition_by = [\"partition_date\"]\n",
    "        else:\n",
    "            # Partition by country for metadata\n",
    "            output_path = f\"{self.bronze_path}/metadata\"\n",
    "            partition_by = [\"country\"]\n",
    "\n",
    "        df.write.mode(\"append\").partitionBy(partition_by).parquet(output_path)\n",
    "\n",
    "    def process_silver_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Process bronze data into silver layer with cleaned and validated data.\n",
    "        Supports incremental processing by date.\n",
    "\n",
    "        Args:\n",
    "            process_date: Optional date to process. If None, processes current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Read only the partition we need from bronze interactions\n",
    "        interactions_df = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.bronze_path}/interactions\"\n",
    "        ).parquet(f\"{self.bronze_path}/interactions/partition_date={process_date}\")\n",
    "\n",
    "        # For metadata, check if we need to process updates\n",
    "        metadata_path = f\"{self.silver_path}/dim_users\"\n",
    "        metadata_df = self.spark.read.parquet(f\"{self.bronze_path}/metadata\")\n",
    "\n",
    "        # Get existing metadata last modified date if exists\n",
    "        try:\n",
    "            existing_metadata = self.spark.read.parquet(metadata_path)\n",
    "            last_modified = existing_metadata.agg(F.max(\"_modified_date\")).collect()[0][\n",
    "                0\n",
    "            ]\n",
    "        except:\n",
    "            last_modified = None\n",
    "            existing_metadata = None\n",
    "            \n",
    "        # Process metadata changes first\n",
    "        if existing_metadata is not None:\n",
    "            # Identify new or updated metadata records\n",
    "            new_metadata_df = metadata_df.withColumn(\n",
    "                \"_modified_date\", F.current_date()\n",
    "            ).join(\n",
    "                existing_metadata, \"user_id\", \"left_anti\"\n",
    "            )  # Get only new records\n",
    "\n",
    "            # Combine existing and new metadata\n",
    "            combined_metadata = existing_metadata.union(new_metadata_df)\n",
    "        else:\n",
    "            combined_metadata = metadata_df.withColumn(\"_modified_date\", F.current_date())\n",
    "            new_metadata_df = combined_metadata\n",
    "\n",
    "        # Process interactions incrementally\n",
    "        clean_interactions = interactions_df.filter(\n",
    "            F.col(\"duration_ms\").between(0, 7200000)\n",
    "        ).dropDuplicates([\"user_id\", \"timestamp\", \"action_type\", \"page_id\"])\n",
    "\n",
    "        # Join with metadata to create enriched fact table\n",
    "        # Broadcast the metadata as it's typically smaller\n",
    "        fact_interactions = (\n",
    "            clean_interactions\n",
    "            .join(\n",
    "                F.broadcast(combined_metadata.select(\n",
    "                    \"user_id\",\n",
    "                    \"country\",\n",
    "                    \"device_type\",\n",
    "                    \"subscription_type\"\n",
    "                )),\n",
    "                \"user_id\",\n",
    "                \"left\"\n",
    "            )\n",
    "            .select(\n",
    "                \"user_id\",\n",
    "                \"timestamp\",\n",
    "                \"action_type\",\n",
    "                \"page_id\",\n",
    "                \"duration_ms\",\n",
    "                \"partition_date\",\n",
    "                \"country\",\n",
    "                \"device_type\",\n",
    "                \"subscription_type\"\n",
    "            )\n",
    "            .withColumn(\"_modified_date\", F.current_date())\n",
    "        )\n",
    "\n",
    "        # Write fact table incrementally\n",
    "        fact_path = f\"{self.silver_path}/fact_interactions\"\n",
    "\n",
    "        (\n",
    "            fact_interactions.write.mode(\"append\")  # Use append mode for incremental\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\n",
    "                \"replaceWhere\", f\"partition_date = '{process_date}'\"\n",
    "            )  # Overwrite only this partition\n",
    "            .parquet(fact_path)\n",
    "        )\n",
    "\n",
    "        # Process metadata changes\n",
    "        if existing_metadata is not None:\n",
    "            # Identify new or updated metadata records\n",
    "            metadata_df = metadata_df.withColumn(\n",
    "                \"_modified_date\", F.current_date()\n",
    "            ).join(\n",
    "                existing_metadata, \"user_id\", \"left_anti\"\n",
    "            )  # Get only new records\n",
    "        else:\n",
    "            metadata_df = metadata_df.withColumn(\"_modified_date\", F.current_date())\n",
    "\n",
    "        if metadata_df.count() > 0:  # Only process if we have changes\n",
    "            dim_users = metadata_df.dropDuplicates([\"user_id\"]).select(\n",
    "                \"user_id\",\n",
    "                \"join_date\",\n",
    "                \"country\",\n",
    "                \"device_type\",\n",
    "                \"subscription_type\",\n",
    "                \"_modified_date\",\n",
    "            )\n",
    "\n",
    "            # Write dimension table\n",
    "            # For small dimension tables, we can use overwrite mode\n",
    "            # For larger ones, consider using merge/upsert operations\n",
    "            if existing_metadata is None:\n",
    "                write_mode = \"overwrite\"\n",
    "            else:\n",
    "                write_mode = \"append\"\n",
    "\n",
    "            (\n",
    "                dim_users.write.mode(write_mode)\n",
    "                .partitionBy(\"country\")\n",
    "                .parquet(metadata_path)\n",
    "            )\n",
    "\n",
    "        # Return metrics about processed data\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"interactions_processed\": clean_interactions.count(),\n",
    "            \"metadata_updates\": metadata_df.count() if metadata_df.count() > 0 else 0,\n",
    "        }\n",
    "\n",
    "    def process_date_range(self, start_date: datetime, end_date: datetime):\n",
    "        \"\"\"Process a range of dates incrementally\"\"\"\n",
    "        current_date = start_date\n",
    "        processing_metrics = []\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            try:\n",
    "                metrics = self.process_silver_layer(current_date)\n",
    "                processing_metrics.append(metrics)\n",
    "                current_date += timedelta(days=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing date {current_date}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        return processing_metrics\n",
    "\n",
    "    def cleanup_old_partitions(self, retention_days: int = 90):\n",
    "        \"\"\"Clean up old partitions based on retention policy\"\"\"\n",
    "        cutoff_date = datetime.now().date() - timedelta(days=retention_days)\n",
    "\n",
    "        # List partitions\n",
    "        bronze_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.bronze_path}/interactions\"\n",
    "        )\n",
    "        silver_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.silver_path}/fact_interactions\"\n",
    "        )\n",
    "\n",
    "        # Delete old partitions\n",
    "        fs = bronze_partitions.getFileSystem(self.spark._jsc.hadoopConfiguration())\n",
    "\n",
    "        for path in [bronze_partitions, silver_partitions]:\n",
    "            if fs.exists(path):\n",
    "                for partition in fs.listStatus(path):\n",
    "                    partition_date = datetime.strptime(\n",
    "                        partition.getPath().getName().split(\"=\")[1], \"%Y-%m-%d\"\n",
    "                    ).date()\n",
    "\n",
    "                    if partition_date < cutoff_date:\n",
    "                        fs.delete(partition.getPath(), True)\n",
    "\n",
    "    def _calculate_session_metrics(\n",
    "        self,\n",
    "        fact_interactions: DataFrame,\n",
    "        process_date: datetime,\n",
    "        lookback_days: int = 1,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate session-based metrics with window functions, handling session boundaries.\n",
    "\n",
    "        Args:\n",
    "            fact_interactions: DataFrame of interactions\n",
    "            process_date: Date to process\n",
    "            lookback_days: Number of days to look back for ongoing sessions\n",
    "        \"\"\"\n",
    "        # Calculate date range for session boundary handling\n",
    "        start_date = process_date - timedelta(days=lookback_days)\n",
    "        end_date = process_date + timedelta(days=1)  # Include full day\n",
    "\n",
    "        # Create window specs without range specification for lag/lead\n",
    "        user_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        # Create window spec for cumulative operations\n",
    "        cumulative_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        sessions_df = (\n",
    "            fact_interactions.filter(\n",
    "                F.col(\"partition_date\").between(start_date, process_date)\n",
    "            )\n",
    "            .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(user_window))\n",
    "            .withColumn(\n",
    "                \"time_diff_minutes\",\n",
    "                F.when(\n",
    "                    F.col(\"prev_timestamp\").isNotNull(),\n",
    "                    (F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\"))\n",
    "                    / 60,\n",
    "                ).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"is_new_session\",\n",
    "                F.when(F.col(\"time_diff_minutes\") >= 30, 1).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_id\",\n",
    "                F.concat(\n",
    "                    F.col(\"user_id\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.date_format(\"partition_date\", \"yyyyMMdd\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.sum(\"is_new_session\").over(cumulative_window),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate metrics only for sessions that end on process_date\n",
    "        return (\n",
    "            sessions_df.withColumn(\n",
    "                \"next_timestamp\", F.lead(\"timestamp\").over(user_window)\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_end\",\n",
    "                F.when(\n",
    "                    F.col(\"next_timestamp\").isNull()\n",
    "                    | (\n",
    "                        (\n",
    "                            F.unix_timestamp(\"next_timestamp\")\n",
    "                            - F.unix_timestamp(\"timestamp\")\n",
    "                        )\n",
    "                        / 60\n",
    "                        >= 30\n",
    "                    ),\n",
    "                    True,\n",
    "                ).otherwise(False),\n",
    "            )\n",
    "            .filter(\n",
    "                (F.col(\"partition_date\") == process_date)\n",
    "                | (F.col(\"session_end\") == True)\n",
    "            )\n",
    "            .groupBy(\"session_id\")\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"actions_per_session\"),\n",
    "                F.sum(\"duration_ms\").alias(\"session_duration_ms\"),\n",
    "                F.first(\"partition_date\").alias(\"session_date\"),\n",
    "                F.last(\"timestamp\").alias(\"session_end_time\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def create_gold_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Create gold layer with pre-aggregated data and business metrics incrementally.\n",
    "\n",
    "        Args:\n",
    "            process_date: Date to process, defaults to current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Load relevant data from silver layer\n",
    "        fact_interactions = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.silver_path}/fact_interactions\"\n",
    "        ).parquet(f\"{self.silver_path}/fact_interactions/partition_date={process_date}\")\n",
    "\n",
    "        dim_users = self.spark.read.parquet(f\"{self.silver_path}/dim_users\")\n",
    "        broadcast_users = F.broadcast(dim_users)\n",
    "\n",
    "        # Calculate daily metrics\n",
    "        daily_metrics = fact_interactions.groupBy(\"partition_date\").agg(\n",
    "            F.countDistinct(\"user_id\").alias(\"daily_active_users\"),\n",
    "            F.count(\"*\").alias(\"total_actions\"),\n",
    "            F.avg(\"duration_ms\").alias(\"avg_duration_ms\"),\n",
    "        )\n",
    "\n",
    "        # Update monthly metrics\n",
    "        month_start = process_date.replace(day=1)\n",
    "        month_end = (process_date + timedelta(days=32)).replace(day=1) - timedelta(\n",
    "            days=1\n",
    "        )\n",
    "\n",
    "        # Read existing monthly metrics for current month if exists\n",
    "        monthly_path = f\"{self.gold_path}/monthly_metrics\"\n",
    "        try:\n",
    "            existing_monthly = self.spark.read.option(\"basePath\", monthly_path).parquet(\n",
    "                f\"{monthly_path}/month_date={month_start}\"\n",
    "            )\n",
    "        except:\n",
    "            existing_monthly = None\n",
    "\n",
    "        # Calculate monthly metrics for current month\n",
    "        month_interactions = (\n",
    "            self.spark.read.option(\"basePath\", f\"{self.silver_path}/fact_interactions\")\n",
    "            .parquet(f\"{self.silver_path}/fact_interactions\")\n",
    "            .filter(F.col(\"partition_date\").between(month_start, month_end))\n",
    "        )\n",
    "\n",
    "        monthly_metrics = (\n",
    "            month_interactions.withColumn(\n",
    "                \"month_date\", F.date_trunc(\"month\", F.col(\"partition_date\"))\n",
    "            )\n",
    "            .groupBy(\"month_date\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"user_id\").alias(\"monthly_active_users\"),\n",
    "                F.count(\"*\").alias(\"total_monthly_actions\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate session metrics with lookback\n",
    "        session_metrics = self._calculate_session_metrics(\n",
    "            fact_interactions, process_date, lookback_days=1\n",
    "        )\n",
    "\n",
    "        # Write metrics to gold layer\n",
    "        # Daily metrics - append mode with partition replacement\n",
    "        (\n",
    "            daily_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\"replaceWhere\", f\"partition_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/daily_metrics\")\n",
    "        )\n",
    "\n",
    "        # Monthly metrics - replace partition for current month\n",
    "        (\n",
    "            monthly_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"month_date\")\n",
    "            .option(\"replaceWhere\", f\"month_date = '{month_start}'\")\n",
    "            .parquet(monthly_path)\n",
    "        )\n",
    "\n",
    "        # Session metrics - append mode with date partitioning\n",
    "        (\n",
    "            session_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"session_date\")\n",
    "            .option(\"replaceWhere\", f\"session_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/session_metrics\")\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"daily_metrics_updated\": daily_metrics.count(),\n",
    "            \"monthly_metrics_updated\": monthly_metrics.count(),\n",
    "            \"sessions_processed\": session_metrics.count(),\n",
    "        }\n",
    "\n",
    "    def backfill_gold_metrics(\n",
    "        self, start_date: datetime, end_date: datetime, parallel: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backfill gold metrics for a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start date for backfill\n",
    "            end_date: End date for backfill\n",
    "            parallel: Whether to process dates in parallel\n",
    "        \"\"\"\n",
    "        if parallel:\n",
    "            # Create list of dates to process\n",
    "            dates = [\n",
    "                (start_date + timedelta(days=x)).date()\n",
    "                for x in range((end_date - start_date).days + 1)\n",
    "            ]\n",
    "\n",
    "            # Process dates in parallel using Spark\n",
    "            date_df = self.spark.createDataFrame(\n",
    "                [(date,) for date in dates], [\"process_date\"]\n",
    "            )\n",
    "\n",
    "            date_df.repartition(min(len(dates), 50)).foreach(\n",
    "                lambda row: self.create_gold_layer(row.process_date)\n",
    "            )\n",
    "        else:\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                try:\n",
    "                    self.create_gold_layer(current_date)\n",
    "                    current_date += timedelta(days=1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {current_date}: {str(e)}\")\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ETL pipeline\n",
    "etl = ETLPipeline(spark)\n",
    "etl.define_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze layer: Ingest and convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bronze layer: Ingest and convert to parquet\n",
    "etl.ingest_to_bronze(\n",
    "    \"s3a://warehouse/data/user_interactions_sample.csv\", \"interactions\"\n",
    ")\n",
    "etl.ingest_to_bronze(\"s3a://warehouse/data/user_metadata_sample.csv\", \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: Clean data and create fact/dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 1),\n",
       " 'interactions_processed': 27488,\n",
       " 'metadata_updates': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Silver layer: Clean data and create fact/dimension tables\n",
    "processing_date = datetime(2023, 1, 1).date()\n",
    "etl.process_silver_layer(processing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: process multiple dates incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 121 ms, sys: 24 ms, total: 146 ms\n",
      "Wall time: 28.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'date_processed': datetime.date(2023, 1, 2),\n",
       "  'interactions_processed': 27530,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 3),\n",
       "  'interactions_processed': 27306,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 4),\n",
       "  'interactions_processed': 27683,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 5),\n",
       "  'interactions_processed': 27364,\n",
       "  'metadata_updates': 0}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "start_date = datetime(2023, 1, 2).date()\n",
    "end_date = datetime(2023, 1, 5).date()\n",
    "etl.process_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- action_type: string (nullable = true)\n",
      " |-- page_id: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- subscription_type: string (nullable = true)\n",
      " |-- _modified_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int = spark.read.parquet(\n",
    "    \"s3a://warehouse/silver/fact_interactions/partition_date=2023-01-03\"\n",
    ")\n",
    "df_fact_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54612"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_int.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------+-----------+-------+--------------+-----------------+--------------+\n",
      "|user_id|          timestamp|action_type|page_id|duration_ms|country|   device_type|subscription_type|_modified_date|\n",
      "+-------+-------------------+-----------+-------+-----------+-------+--------------+-----------------+--------------+\n",
      "|u751627|2023-01-03 17:45:22|       edit|p621585|     118746|     JP|Android Tablet|            basic|    2024-12-20|\n",
      "|u491077|2023-01-03 15:38:30|     create|p265882|     160165|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u807778|2023-01-03 09:56:58|      share|p554854|     151881|     BR|           Mac|          premium|    2024-12-20|\n",
      "|u346752|2023-01-03 00:18:16|       edit|p949965|      56082|     IN|        iPhone|       enterprise|    2024-12-20|\n",
      "|u595276|2023-01-03 19:16:12|  page_view|p224316|     264913|     MX|          iPad|          premium|    2024-12-20|\n",
      "|u239288|2023-01-03 17:12:01|  page_view|p525366|     295865|     AU|        iPhone|       enterprise|    2024-12-20|\n",
      "|u517928|2023-01-03 00:12:13|       edit|p455761|     125278|     IN|        iPhone|       enterprise|    2024-12-20|\n",
      "|u381446|2023-01-03 10:58:55|     create|p944592|       6746|     IN|        iPhone|          premium|    2024-12-20|\n",
      "|u060346|2023-01-03 01:41:20|      share|p565179|      39583|     DE|           Mac|            basic|    2024-12-20|\n",
      "|u857070|2023-01-03 05:30:58|     delete|p698150|     100001|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u070581|2023-01-03 20:26:34|     delete|p440116|     150082|     BR|           Mac|       enterprise|    2024-12-20|\n",
      "|u401973|2023-01-03 00:58:01|  page_view|p813501|      17748|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u631822|2023-01-03 02:32:36|  page_view|p724144|     267265|     MX|        iPhone|       enterprise|    2024-12-20|\n",
      "|u678522|2023-01-03 00:07:25|  page_view|p319616|     263001|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u186544|2023-01-03 06:36:04|     delete|p112684|        560|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u163750|2023-01-03 08:50:30|     create|p367907|     135534|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u253530|2023-01-03 03:52:27|       edit|p414820|     152694|   NULL|          NULL|             NULL|    2024-12-20|\n",
      "|u428243|2023-01-03 08:40:07|       edit|p184615|     243690|     US|       Windows|       enterprise|    2024-12-20|\n",
      "|u970877|2023-01-03 03:33:26|     create|p208129|     207415|     BR|Android Tablet|             free|    2024-12-20|\n",
      "|u559282|2023-01-03 09:52:54|     create|p415491|     274710|     UK|       Windows|       enterprise|    2024-12-20|\n",
      "+-------+-------------------+-----------+-------+-----------+-------+--------------+-----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold layer: Create pre-aggregated business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 5),\n",
       " 'daily_metrics_updated': 1,\n",
       " 'monthly_metrics_updated': 1,\n",
       " 'sessions_processed': 27344}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process single date\n",
    "processing_date = datetime(2023, 1, 5).date()\n",
    "metrics = etl.create_gold_layer(processing_date)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- actions_per_session: long (nullable = true)\n",
      " |-- session_duration_ms: long (nullable = true)\n",
      " |-- session_end_time: timestamp (nullable = true)\n",
      " |-- session_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics = spark.read.parquet(\"s3a://warehouse/gold/session_metrics/\")\n",
    "df_sess_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|        session_id|actions_per_session|session_duration_ms|   session_end_time|session_date|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|u004300_20230105_0|                  2|              94148|2023-01-05 05:57:05|  2023-01-05|\n",
      "|u004546_20230105_0|                  2|              97398|2023-01-05 16:33:54|  2023-01-05|\n",
      "|u004691_20230105_0|                  2|             583716|2023-01-05 07:32:53|  2023-01-05|\n",
      "|u004814_20230105_0|                  2|             301572|2023-01-05 20:53:04|  2023-01-05|\n",
      "|u016776_20230105_0|                  2|             288734|2023-01-05 05:15:32|  2023-01-05|\n",
      "|u037646_20230105_0|                  2|             536454|2023-01-05 03:46:44|  2023-01-05|\n",
      "|u049417_20230105_0|                  2|             269394|2023-01-05 13:18:47|  2023-01-05|\n",
      "|u064662_20230105_0|                  2|             419944|2023-01-05 09:55:33|  2023-01-05|\n",
      "|u070860_20230105_0|                  2|              50296|2023-01-05 13:01:00|  2023-01-05|\n",
      "|u073014_20230105_0|                  2|             367800|2023-01-05 08:14:55|  2023-01-05|\n",
      "|u079858_20230105_0|                  2|             467344|2023-01-05 05:38:19|  2023-01-05|\n",
      "|u100540_20230105_0|                  2|             522568|2023-01-05 21:05:36|  2023-01-05|\n",
      "|u101880_20230105_0|                  2|             587094|2023-01-05 13:10:39|  2023-01-05|\n",
      "|u104871_20230105_0|                  2|             228506|2023-01-05 03:26:38|  2023-01-05|\n",
      "|u107851_20230105_0|                  2|             351608|2023-01-05 12:06:08|  2023-01-05|\n",
      "|u112067_20230105_0|                  2|             448276|2023-01-05 19:56:07|  2023-01-05|\n",
      "|u127029_20230105_0|                  2|             537904|2023-01-05 17:15:15|  2023-01-05|\n",
      "|u142995_20230105_0|                  2|             143940|2023-01-05 02:22:00|  2023-01-05|\n",
      "|u154119_20230105_0|                  2|             137304|2023-01-05 22:48:34|  2023-01-05|\n",
      "|u158242_20230105_0|                  2|             154000|2023-01-05 04:03:58|  2023-01-05|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
