{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    LongType,\n",
    "    DateType,\n",
    ")\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 10:44:26 WARN Utils: Your hostname, hongong-predator resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/12/19 10:44:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/hongong/miniconda3/envs/goodnotes-insights-data-eng/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/hongong/.ivy2/cache\n",
      "The jars for the packages stored in: /home/hongong/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-azure added as a dependency\n",
      "com.microsoft.azure#azure-storage added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-08f04b03-3dbc-4a92-843d-966ade5785df;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.3.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.40.v20210413 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound com.microsoft.azure#azure-storage;8.6.6 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.12 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.4 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.2.4 in central\n",
      "\tfound com.google.guava#guava;24.1.1-jre in central\n",
      "\tfound com.google.code.findbugs#jsr305;1.3.9 in central\n",
      "\tfound org.checkerframework#checker-compat-qual;2.0.0 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.1.3 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.14 in central\n",
      ":: resolution report :: resolve 497ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.4 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;1.3.9 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.1.3 from central in [default]\n",
      "\tcom.google.guava#guava;24.1.1-jre from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.2.4 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;8.6.6 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.checkerframework#checker-compat-qual;2.0.0 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.14 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.40.v20210413 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.40.v20210413 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.12 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 by [com.microsoft.azure#azure-storage;8.6.6] in [default]\n",
      "\torg.apache.commons#commons-lang3;3.8.1 by [org.apache.commons#commons-lang3;3.4] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   2   ||   22  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-08f04b03-3dbc-4a92-843d-966ade5785df\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 22 already retrieved (0kB/11ms)\n",
      "24/12/19 10:44:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(name=\"unittest\")\n",
    "    .config(\"spark.ui.enabled\", \"false\")\n",
    "    .config(\n",
    "        \"spark.driver.extraJavaOptions\",\n",
    "        \"--add-exports java.base/sun.nio.ch=ALL-UNNAMED\",\n",
    "    )\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.hadoop:hadoop-azure:3.3.1,com.microsoft.azure:azure-storage:8.6.6\",\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    def __init__(self, spark):\n",
    "        self.bucket_name = f\"noteapp\"\n",
    "        self.spark = spark\n",
    "\n",
    "        # Define storage paths\n",
    "        self.bronze_path = f\"{self.bucket_name}/bronze\"\n",
    "        self.silver_path = f\"{self.bucket_name}/silver\"\n",
    "        self.gold_path = f\"{self.bucket_name}/gold\"\n",
    "\n",
    "    def define_schemas(self):\n",
    "        \"\"\"Define schemas for the datasets\"\"\"\n",
    "        self.interactions_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"timestamp\", TimestampType(), False),\n",
    "                StructField(\"action_type\", StringType(), False),\n",
    "                StructField(\"page_id\", StringType(), False),\n",
    "                StructField(\"duration_ms\", LongType(), False),\n",
    "                StructField(\"app_version\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.metadata_schema = StructType(\n",
    "            [\n",
    "                StructField(\"user_id\", StringType(), False),\n",
    "                StructField(\"join_date\", DateType(), False),\n",
    "                StructField(\"country\", StringType(), False),\n",
    "                StructField(\"device_type\", StringType(), False),\n",
    "                StructField(\"subscription_type\", StringType(), False),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def ingest_to_bronze(self, csv_path: str, dataset_type: str):\n",
    "        \"\"\"Ingest CSV files to bronze layer in parquet format\"\"\"\n",
    "        schema = (\n",
    "            self.interactions_schema\n",
    "            if dataset_type == \"interactions\"\n",
    "            else self.metadata_schema\n",
    "        )\n",
    "\n",
    "        df = self.spark.read.schema(schema).csv(csv_path)\n",
    "\n",
    "        if dataset_type == \"interactions\":\n",
    "            # Partition by date for interactions\n",
    "            df = df.withColumn(\"partition_date\", F.to_date(\"timestamp\"))\n",
    "            output_path = f\"{self.bronze_path}/interactions\"\n",
    "            partition_by = [\"partition_date\"]\n",
    "        else:\n",
    "            # Partition by country for metadata\n",
    "            output_path = f\"{self.bronze_path}/metadata\"\n",
    "            partition_by = [\"country\"]\n",
    "\n",
    "        df.write.mode(\"append\").partitionBy(partition_by).parquet(output_path)\n",
    "\n",
    "    def process_silver_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Process bronze data into silver layer with cleaned and validated data.\n",
    "        Supports incremental processing by date.\n",
    "\n",
    "        Args:\n",
    "            process_date: Optional date to process. If None, processes current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Read only the partition we need from bronze interactions\n",
    "        interactions_df = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.bronze_path}/interactions\"\n",
    "        ).parquet(f\"{self.bronze_path}/interactions/partition_date={process_date}\")\n",
    "\n",
    "        # For metadata, check if we need to process updates\n",
    "        metadata_path = f\"{self.silver_path}/dim_users\"\n",
    "        metadata_df = self.spark.read.parquet(f\"{self.bronze_path}/metadata\")\n",
    "\n",
    "        # Get existing metadata last modified date if exists\n",
    "        try:\n",
    "            existing_metadata = self.spark.read.parquet(metadata_path)\n",
    "            last_modified = existing_metadata.agg(F.max(\"_modified_date\")).collect()[0][\n",
    "                0\n",
    "            ]\n",
    "        except:\n",
    "            last_modified = None\n",
    "            existing_metadata = None\n",
    "\n",
    "        # Process interactions incrementally\n",
    "        clean_interactions = interactions_df.filter(\n",
    "            F.col(\"duration_ms\").between(0, 7200000)\n",
    "        ).dropDuplicates([\"user_id\", \"timestamp\", \"action_type\", \"page_id\"])\n",
    "\n",
    "        fact_interactions = clean_interactions.select(\n",
    "            \"user_id\",\n",
    "            \"timestamp\",\n",
    "            \"action_type\",\n",
    "            \"page_id\",\n",
    "            \"duration_ms\",\n",
    "            \"partition_date\",\n",
    "        ).withColumn(\"_modified_date\", F.current_date())\n",
    "\n",
    "        # Write fact table incrementally\n",
    "        fact_path = f\"{self.silver_path}/fact_interactions\"\n",
    "\n",
    "        (\n",
    "            fact_interactions.write.mode(\"append\")  # Use append mode for incremental\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\n",
    "                \"replaceWhere\", f\"partition_date = '{process_date}'\"\n",
    "            )  # Overwrite only this partition\n",
    "            .parquet(fact_path)\n",
    "        )\n",
    "\n",
    "        # Process metadata changes\n",
    "        if existing_metadata is not None:\n",
    "            # Identify new or updated metadata records\n",
    "            metadata_df = metadata_df.withColumn(\n",
    "                \"_modified_date\", F.current_date()\n",
    "            ).join(\n",
    "                existing_metadata, \"user_id\", \"left_anti\"\n",
    "            )  # Get only new records\n",
    "        else:\n",
    "            metadata_df = metadata_df.withColumn(\"_modified_date\", F.current_date())\n",
    "\n",
    "        if metadata_df.count() > 0:  # Only process if we have changes\n",
    "            dim_users = metadata_df.dropDuplicates([\"user_id\"]).select(\n",
    "                \"user_id\",\n",
    "                \"join_date\",\n",
    "                \"country\",\n",
    "                \"device_type\",\n",
    "                \"subscription_type\",\n",
    "                \"_modified_date\",\n",
    "            )\n",
    "\n",
    "            # Write dimension table\n",
    "            # For small dimension tables, we can use overwrite mode\n",
    "            # For larger ones, consider using merge/upsert operations\n",
    "            if existing_metadata is None:\n",
    "                write_mode = \"overwrite\"\n",
    "            else:\n",
    "                write_mode = \"append\"\n",
    "\n",
    "            (\n",
    "                dim_users.write.mode(write_mode)\n",
    "                .partitionBy(\"country\")\n",
    "                .parquet(metadata_path)\n",
    "            )\n",
    "\n",
    "        # Return metrics about processed data\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"interactions_processed\": clean_interactions.count(),\n",
    "            \"metadata_updates\": metadata_df.count() if metadata_df.count() > 0 else 0,\n",
    "        }\n",
    "\n",
    "    def process_date_range(self, start_date: datetime, end_date: datetime):\n",
    "        \"\"\"Process a range of dates incrementally\"\"\"\n",
    "        current_date = start_date\n",
    "        processing_metrics = []\n",
    "\n",
    "        while current_date <= end_date:\n",
    "            try:\n",
    "                metrics = self.process_silver_layer(current_date)\n",
    "                processing_metrics.append(metrics)\n",
    "                current_date += timedelta(days=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing date {current_date}: {str(e)}\")\n",
    "                raise\n",
    "\n",
    "        return processing_metrics\n",
    "\n",
    "    def cleanup_old_partitions(self, retention_days: int = 90):\n",
    "        \"\"\"Clean up old partitions based on retention policy\"\"\"\n",
    "        cutoff_date = datetime.now().date() - timedelta(days=retention_days)\n",
    "\n",
    "        # List partitions\n",
    "        bronze_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.bronze_path}/interactions\"\n",
    "        )\n",
    "        silver_partitions = self.spark._jvm.org.apache.hadoop.fs.Path(\n",
    "            f\"{self.silver_path}/fact_interactions\"\n",
    "        )\n",
    "\n",
    "        # Delete old partitions\n",
    "        fs = bronze_partitions.getFileSystem(self.spark._jsc.hadoopConfiguration())\n",
    "\n",
    "        for path in [bronze_partitions, silver_partitions]:\n",
    "            if fs.exists(path):\n",
    "                for partition in fs.listStatus(path):\n",
    "                    partition_date = datetime.strptime(\n",
    "                        partition.getPath().getName().split(\"=\")[1], \"%Y-%m-%d\"\n",
    "                    ).date()\n",
    "\n",
    "                    if partition_date < cutoff_date:\n",
    "                        fs.delete(partition.getPath(), True)\n",
    "\n",
    "    def _calculate_session_metrics(\n",
    "        self,\n",
    "        fact_interactions: DataFrame,\n",
    "        process_date: datetime,\n",
    "        lookback_days: int = 1,\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate session-based metrics with window functions, handling session boundaries.\n",
    "\n",
    "        Args:\n",
    "            fact_interactions: DataFrame of interactions\n",
    "            process_date: Date to process\n",
    "            lookback_days: Number of days to look back for ongoing sessions\n",
    "        \"\"\"\n",
    "        # Calculate date range for session boundary handling\n",
    "        start_date = process_date - timedelta(days=lookback_days)\n",
    "        end_date = process_date + timedelta(days=1)  # Include full day\n",
    "\n",
    "        # Create window specs without range specification for lag/lead\n",
    "        user_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        # Create window spec for cumulative operations\n",
    "        cumulative_window = Window.partitionBy(\"user_id\").orderBy(\"timestamp\")\n",
    "\n",
    "        sessions_df = (\n",
    "            fact_interactions.filter(\n",
    "                F.col(\"partition_date\").between(start_date, process_date)\n",
    "            )\n",
    "            .withColumn(\"prev_timestamp\", F.lag(\"timestamp\").over(user_window))\n",
    "            .withColumn(\n",
    "                \"time_diff_minutes\",\n",
    "                F.when(\n",
    "                    F.col(\"prev_timestamp\").isNotNull(),\n",
    "                    (F.unix_timestamp(\"timestamp\") - F.unix_timestamp(\"prev_timestamp\"))\n",
    "                    / 60,\n",
    "                ).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"is_new_session\",\n",
    "                F.when(F.col(\"time_diff_minutes\") >= 30, 1).otherwise(0),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_id\",\n",
    "                F.concat(\n",
    "                    F.col(\"user_id\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.date_format(\"partition_date\", \"yyyyMMdd\"),\n",
    "                    F.lit(\"_\"),\n",
    "                    F.sum(\"is_new_session\").over(cumulative_window),\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate metrics only for sessions that end on process_date\n",
    "        return (\n",
    "            sessions_df.withColumn(\n",
    "                \"next_timestamp\", F.lead(\"timestamp\").over(user_window)\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"session_end\",\n",
    "                F.when(\n",
    "                    F.col(\"next_timestamp\").isNull()\n",
    "                    | (\n",
    "                        (\n",
    "                            F.unix_timestamp(\"next_timestamp\")\n",
    "                            - F.unix_timestamp(\"timestamp\")\n",
    "                        )\n",
    "                        / 60\n",
    "                        >= 30\n",
    "                    ),\n",
    "                    True,\n",
    "                ).otherwise(False),\n",
    "            )\n",
    "            .filter(\n",
    "                (F.col(\"partition_date\") == process_date)\n",
    "                | (F.col(\"session_end\") == True)\n",
    "            )\n",
    "            .groupBy(\"session_id\")\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"actions_per_session\"),\n",
    "                F.sum(\"duration_ms\").alias(\"session_duration_ms\"),\n",
    "                F.first(\"partition_date\").alias(\"session_date\"),\n",
    "                F.last(\"timestamp\").alias(\"session_end_time\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def create_gold_layer(self, process_date: datetime = None):\n",
    "        \"\"\"\n",
    "        Create gold layer with pre-aggregated data and business metrics incrementally.\n",
    "\n",
    "        Args:\n",
    "            process_date: Date to process, defaults to current date\n",
    "        \"\"\"\n",
    "        if process_date is None:\n",
    "            process_date = datetime.now().date()\n",
    "\n",
    "        # Load relevant data from silver layer\n",
    "        fact_interactions = self.spark.read.option(\n",
    "            \"basePath\", f\"{self.silver_path}/fact_interactions\"\n",
    "        ).parquet(f\"{self.silver_path}/fact_interactions/partition_date={process_date}\")\n",
    "\n",
    "        dim_users = self.spark.read.parquet(f\"{self.silver_path}/dim_users\")\n",
    "        broadcast_users = F.broadcast(dim_users)\n",
    "\n",
    "        # Calculate daily metrics\n",
    "        daily_metrics = fact_interactions.groupBy(\"partition_date\").agg(\n",
    "            F.countDistinct(\"user_id\").alias(\"daily_active_users\"),\n",
    "            F.count(\"*\").alias(\"total_actions\"),\n",
    "            F.avg(\"duration_ms\").alias(\"avg_duration_ms\"),\n",
    "        )\n",
    "\n",
    "        # Update monthly metrics\n",
    "        month_start = process_date.replace(day=1)\n",
    "        month_end = (process_date + timedelta(days=32)).replace(day=1) - timedelta(\n",
    "            days=1\n",
    "        )\n",
    "\n",
    "        # Read existing monthly metrics for current month if exists\n",
    "        monthly_path = f\"{self.gold_path}/monthly_metrics\"\n",
    "        try:\n",
    "            existing_monthly = self.spark.read.option(\"basePath\", monthly_path).parquet(\n",
    "                f\"{monthly_path}/month_date={month_start}\"\n",
    "            )\n",
    "        except:\n",
    "            existing_monthly = None\n",
    "\n",
    "        # Calculate monthly metrics for current month\n",
    "        month_interactions = (\n",
    "            self.spark.read.option(\"basePath\", f\"{self.silver_path}/fact_interactions\")\n",
    "            .parquet(f\"{self.silver_path}/fact_interactions\")\n",
    "            .filter(F.col(\"partition_date\").between(month_start, month_end))\n",
    "        )\n",
    "\n",
    "        monthly_metrics = (\n",
    "            month_interactions.withColumn(\n",
    "                \"month_date\", F.date_trunc(\"month\", F.col(\"partition_date\"))\n",
    "            )\n",
    "            .groupBy(\"month_date\")\n",
    "            .agg(\n",
    "                F.countDistinct(\"user_id\").alias(\"monthly_active_users\"),\n",
    "                F.count(\"*\").alias(\"total_monthly_actions\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Calculate session metrics with lookback\n",
    "        session_metrics = self._calculate_session_metrics(\n",
    "            fact_interactions, process_date, lookback_days=1\n",
    "        )\n",
    "\n",
    "        # Write metrics to gold layer\n",
    "        # Daily metrics - append mode with partition replacement\n",
    "        (\n",
    "            daily_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"partition_date\")\n",
    "            .option(\"replaceWhere\", f\"partition_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/daily_metrics\")\n",
    "        )\n",
    "\n",
    "        # Monthly metrics - replace partition for current month\n",
    "        (\n",
    "            monthly_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"month_date\")\n",
    "            .option(\"replaceWhere\", f\"month_date = '{month_start}'\")\n",
    "            .parquet(monthly_path)\n",
    "        )\n",
    "\n",
    "        # Session metrics - append mode with date partitioning\n",
    "        (\n",
    "            session_metrics.write.mode(\"append\")\n",
    "            .partitionBy(\"session_date\")\n",
    "            .option(\"replaceWhere\", f\"session_date = '{process_date}'\")\n",
    "            .parquet(f\"{self.gold_path}/session_metrics\")\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"date_processed\": process_date,\n",
    "            \"daily_metrics_updated\": daily_metrics.count(),\n",
    "            \"monthly_metrics_updated\": monthly_metrics.count(),\n",
    "            \"sessions_processed\": session_metrics.count(),\n",
    "        }\n",
    "\n",
    "    def backfill_gold_metrics(\n",
    "        self, start_date: datetime, end_date: datetime, parallel: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Backfill gold metrics for a date range.\n",
    "\n",
    "        Args:\n",
    "            start_date: Start date for backfill\n",
    "            end_date: End date for backfill\n",
    "            parallel: Whether to process dates in parallel\n",
    "        \"\"\"\n",
    "        if parallel:\n",
    "            # Create list of dates to process\n",
    "            dates = [\n",
    "                (start_date + timedelta(days=x)).date()\n",
    "                for x in range((end_date - start_date).days + 1)\n",
    "            ]\n",
    "\n",
    "            # Process dates in parallel using Spark\n",
    "            date_df = self.spark.createDataFrame(\n",
    "                [(date,) for date in dates], [\"process_date\"]\n",
    "            )\n",
    "\n",
    "            date_df.repartition(min(len(dates), 50)).foreach(\n",
    "                lambda row: self.create_gold_layer(row.process_date)\n",
    "            )\n",
    "        else:\n",
    "            current_date = start_date\n",
    "            while current_date <= end_date:\n",
    "                try:\n",
    "                    self.create_gold_layer(current_date)\n",
    "                    current_date += timedelta(days=1)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {current_date}: {str(e)}\")\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ETL pipeline\n",
    "etl = ETLPipeline(spark)\n",
    "etl.define_schemas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze layer: Ingest and convert to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/19 10:28:04 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bronze layer: Ingest and convert to parquet\n",
    "etl.ingest_to_bronze(\"data/user_interactions_sample.csv\", \"interactions\")\n",
    "etl.ingest_to_bronze(\"data/user_metadata_sample.csv\", \"metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1,5M\n",
      "drwxr-xr-x 367 hongong hongong  28K Thg 12 19 10:28 .\n",
      "drwxr-xr-x   4 hongong hongong 4,0K Thg 12 19 10:28 ..\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-01\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-02\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-03\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-04\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-05\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-06\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 partition_date=2023-01-07\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/bronze/interactions | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 184K\n",
      "drwxr-xr-x   2 hongong hongong 4,0K Thg 12 19 10:28 .\n",
      "drwxr-xr-x 367 hongong hongong  28K Thg 12 19 10:28 ..\n",
      "-rw-r--r--   1 hongong hongong  14K Thg 12 19 10:28 part-00000-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  116 Thg 12 19 10:28 .part-00000-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  13K Thg 12 19 10:28 part-00001-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  112 Thg 12 19 10:28 .part-00001-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  14K Thg 12 19 10:28 part-00002-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  120 Thg 12 19 10:28 .part-00002-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  14K Thg 12 19 10:28 part-00003-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  116 Thg 12 19 10:28 .part-00003-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  14K Thg 12 19 10:28 part-00004-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  116 Thg 12 19 10:28 .part-00004-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  14K Thg 12 19 10:28 part-00005-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  116 Thg 12 19 10:28 .part-00005-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong  13K Thg 12 19 10:28 part-00006-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong  108 Thg 12 19 10:28 .part-00006-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n",
      "-rw-r--r--   1 hongong hongong 6,7K Thg 12 19 10:28 part-00007-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet\n",
      "-rw-r--r--   1 hongong hongong   64 Thg 12 19 10:28 .part-00007-be2c411e-d248-43f3-a6af-4756d6a0e5d8.c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/bronze/interactions/partition_date=2023-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\n",
      "drwxr-xr-x 13 hongong hongong 4,0K Thg 12 19 10:28 .\n",
      "drwxr-xr-x  4 hongong hongong 4,0K Thg 12 19 10:28 ..\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=AU\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=BR\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=CA\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=country\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=DE\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=FR\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 country=IN\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/bronze/metadata | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 104K\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:28 .\n",
      "drwxr-xr-x 13 hongong hongong 4,0K Thg 12 19 10:28 ..\n",
      "-rw-r--r--  1 hongong hongong  89K Thg 12 19 10:28 part-00000-536bb221-ff69-4025-9f45-72360c5cfde0.c000.snappy.parquet\n",
      "-rw-r--r--  1 hongong hongong  716 Thg 12 19 10:28 .part-00000-536bb221-ff69-4025-9f45-72360c5cfde0.c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/bronze/metadata/country=AU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: Clean data and create fact/dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 1),\n",
       " 'interactions_processed': 2731,\n",
       " 'metadata_updates': 100001}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Silver layer: Clean data and create fact/dimension tables\n",
    "processing_date = datetime(2023, 1, 1).date()\n",
    "etl.process_silver_layer(processing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56K\n",
      "drwxr-xr-x 13 hongong hongong 4,0K Thg 12 19 10:48  .\n",
      "drwxr-xr-x  4 hongong hongong 4,0K Thg 12 19 10:48  ..\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=AU'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=BR'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=CA'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=country'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=DE'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=FR'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=IN'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=JP'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=MX'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=UK'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'country=US'\n",
      "-rw-r--r--  1 hongong hongong    0 Thg 12 19 10:48  _SUCCESS\n",
      "-rw-r--r--  1 hongong hongong    8 Thg 12 19 10:48  ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/silver/dim_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 10:48  .\n",
      "drwxr-xr-x 4 hongong hongong 4,0K Thg 12 19 10:48  ..\n",
      "drwxr-xr-x 2 hongong hongong 4,0K Thg 12 19 10:48 'partition_date=2023-01-01'\n",
      "-rw-r--r-- 1 hongong hongong    0 Thg 12 19 10:48  _SUCCESS\n",
      "-rw-r--r-- 1 hongong hongong    8 Thg 12 19 10:48  ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/silver/fact_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 88K\n",
      "drwxr-xr-x 2 hongong hongong 4,0K Thg 12 19 10:48 .\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 10:48 ..\n",
      "-rw-r--r-- 1 hongong hongong  74K Thg 12 19 10:48 part-00000-9090108e-7af3-4a92-b1f5-7d1b48460395.c000.snappy.parquet\n",
      "-rw-r--r-- 1 hongong hongong  596 Thg 12 19 10:48 .part-00000-9090108e-7af3-4a92-b1f5-7d1b48460395.c000.snappy.parquet.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/silver/fact_interactions/partition_date=2023-01-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver layer: process multiple dates incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'date_processed': datetime.date(2023, 1, 2),\n",
       "  'interactions_processed': 2742,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 3),\n",
       "  'interactions_processed': 2696,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 4),\n",
       "  'interactions_processed': 2795,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 5),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 6),\n",
       "  'interactions_processed': 2809,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 7),\n",
       "  'interactions_processed': 2648,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 8),\n",
       "  'interactions_processed': 2771,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 9),\n",
       "  'interactions_processed': 2800,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 10),\n",
       "  'interactions_processed': 2780,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 11),\n",
       "  'interactions_processed': 2827,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 12),\n",
       "  'interactions_processed': 2737,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 13),\n",
       "  'interactions_processed': 2876,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 14),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 15),\n",
       "  'interactions_processed': 2748,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 16),\n",
       "  'interactions_processed': 2793,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 17),\n",
       "  'interactions_processed': 2694,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 18),\n",
       "  'interactions_processed': 2774,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 19),\n",
       "  'interactions_processed': 2673,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 20),\n",
       "  'interactions_processed': 2720,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 21),\n",
       "  'interactions_processed': 2776,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 22),\n",
       "  'interactions_processed': 2779,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 23),\n",
       "  'interactions_processed': 2695,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 24),\n",
       "  'interactions_processed': 2790,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 25),\n",
       "  'interactions_processed': 2751,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 26),\n",
       "  'interactions_processed': 2794,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 27),\n",
       "  'interactions_processed': 2717,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 28),\n",
       "  'interactions_processed': 2698,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 29),\n",
       "  'interactions_processed': 2867,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 30),\n",
       "  'interactions_processed': 2721,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 1, 31),\n",
       "  'interactions_processed': 2764,\n",
       "  'metadata_updates': 0},\n",
       " {'date_processed': datetime.date(2023, 2, 1),\n",
       "  'interactions_processed': 2792,\n",
       "  'metadata_updates': 0}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = datetime(2023, 1, 2).date()\n",
    "end_date = datetime(2023, 2, 1).date()\n",
    "etl.process_date_range(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 140K\n",
      "drwxr-xr-x 34 hongong hongong 4,0K Thg 12 19 11:12  .\n",
      "drwxr-xr-x  4 hongong hongong 4,0K Thg 12 19 10:48  ..\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 10:48 'partition_date=2023-01-01'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-02'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-03'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-04'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-05'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-06'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-07'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-08'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-09'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-10'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-11'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-12'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-13'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-14'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-15'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-16'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-17'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-18'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-19'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-20'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-21'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-22'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-23'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-24'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-25'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-26'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-27'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-28'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-29'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-30'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-01-31'\n",
      "drwxr-xr-x  2 hongong hongong 4,0K Thg 12 19 11:12 'partition_date=2023-02-01'\n",
      "-rw-r--r--  1 hongong hongong    0 Thg 12 19 11:12  _SUCCESS\n",
      "-rw-r--r--  1 hongong hongong    8 Thg 12 19 11:12  ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/silver/fact_interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- action_type: string (nullable = true)\n",
      " |-- page_id: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- _modified_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int = spark.read.parquet(\n",
    "    \"noteapp/silver/fact_interactions/partition_date=2023-01-03\"\n",
    ")\n",
    "df_fact_int.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2696"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_int.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "|user_id|          timestamp|action_type|page_id|duration_ms|_modified_date|\n",
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "|u880939|2023-01-03 12:28:38|      share|p891876|     284189|    2024-12-19|\n",
      "|u483517|2023-01-03 14:01:43|      share|p207145|     101050|    2024-12-19|\n",
      "|u647684|2023-01-03 04:29:05|       edit|p327225|      19413|    2024-12-19|\n",
      "|u531187|2023-01-03 12:55:55|       edit|p370054|     152019|    2024-12-19|\n",
      "|u080903|2023-01-03 16:55:57|       edit|p709243|      46027|    2024-12-19|\n",
      "|u173160|2023-01-03 07:20:39|     delete|p512034|     135144|    2024-12-19|\n",
      "|u126441|2023-01-03 19:43:41|     create|p500152|     256895|    2024-12-19|\n",
      "|u567436|2023-01-03 01:06:09|       edit|p484945|     252431|    2024-12-19|\n",
      "|u496595|2023-01-03 22:33:29|      share|p229867|      10619|    2024-12-19|\n",
      "|u179135|2023-01-03 21:47:15|     create|p224419|     197715|    2024-12-19|\n",
      "|u624056|2023-01-03 21:25:53|  page_view|p312486|     292239|    2024-12-19|\n",
      "|u329995|2023-01-03 23:45:09|       edit|p076793|      78377|    2024-12-19|\n",
      "|u860419|2023-01-03 23:19:54|       edit|p560709|     236298|    2024-12-19|\n",
      "|u694155|2023-01-03 09:55:59|     create|p798437|     120128|    2024-12-19|\n",
      "|u417692|2023-01-03 18:58:18|      share|p824973|      87573|    2024-12-19|\n",
      "|u923994|2023-01-03 07:50:24|      share|p993681|     221842|    2024-12-19|\n",
      "|u931733|2023-01-03 12:54:38|       edit|p945046|      19921|    2024-12-19|\n",
      "|u097414|2023-01-03 05:16:52|      share|p558613|     284766|    2024-12-19|\n",
      "|u662517|2023-01-03 06:52:24|       edit|p483873|     216061|    2024-12-19|\n",
      "|u932956|2023-01-03 02:02:24|     create|p262682|     297908|    2024-12-19|\n",
      "+-------+-------------------+-----------+-------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_int.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold layer: Create pre-aggregated business metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date_processed': datetime.date(2023, 1, 15),\n",
       " 'daily_metrics_updated': 1,\n",
       " 'monthly_metrics_updated': 1,\n",
       " 'sessions_processed': 2746}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process single date\n",
    "processing_date = datetime(2023, 1, 15).date()\n",
    "metrics = etl.create_gold_layer(processing_date)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20K\n",
      "drwxr-xr-x 5 hongong hongong 4,0K Thg 12 19 11:27 .\n",
      "drwxr-xr-x 5 hongong hongong 4,0K Thg 12 19 11:27 ..\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 11:27 daily_metrics\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 11:27 monthly_metrics\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 11:27 session_metrics\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 16K\n",
      "drwxr-xr-x 3 hongong hongong 4,0K Thg 12 19 11:27  .\n",
      "drwxr-xr-x 5 hongong hongong 4,0K Thg 12 19 11:27  ..\n",
      "drwxr-xr-x 2 hongong hongong 4,0K Thg 12 19 11:27 'session_date=2023-01-15'\n",
      "-rw-r--r-- 1 hongong hongong    0 Thg 12 19 11:27  _SUCCESS\n",
      "-rw-r--r-- 1 hongong hongong    8 Thg 12 19 11:27  ._SUCCESS.crc\n"
     ]
    }
   ],
   "source": [
    "!ls -lah noteapp/gold/session_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- actions_per_session: long (nullable = true)\n",
      " |-- session_duration_ms: long (nullable = true)\n",
      " |-- session_end_time: timestamp (nullable = true)\n",
      " |-- session_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics = spark.read.parquet(\"noteapp/gold/session_metrics/\")\n",
    "df_sess_metrics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|        session_id|actions_per_session|session_duration_ms|   session_end_time|session_date|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "|u150209_20230115_0|                  1|             235478|2023-01-15 11:39:30|  2023-01-15|\n",
      "|u212067_20230115_0|                  1|              82707|2023-01-15 06:16:18|  2023-01-15|\n",
      "|u294155_20230115_0|                  1|              12907|2023-01-15 21:46:11|  2023-01-15|\n",
      "|u296373_20230115_0|                  1|             222310|2023-01-15 08:49:20|  2023-01-15|\n",
      "|u308654_20230115_0|                  1|              69602|2023-01-15 12:10:26|  2023-01-15|\n",
      "|u384261_20230115_0|                  1|             270021|2023-01-15 13:40:07|  2023-01-15|\n",
      "|u399815_20230115_0|                  1|             290451|2023-01-15 10:47:36|  2023-01-15|\n",
      "|u482058_20230115_0|                  1|              28435|2023-01-15 10:34:54|  2023-01-15|\n",
      "|u518961_20230115_0|                  1|             168388|2023-01-15 22:56:30|  2023-01-15|\n",
      "|u528951_20230115_0|                  1|             266728|2023-01-15 07:36:18|  2023-01-15|\n",
      "|u609858_20230115_0|                  1|             270067|2023-01-15 15:59:02|  2023-01-15|\n",
      "|u616504_20230115_0|                  1|               4966|2023-01-15 03:03:13|  2023-01-15|\n",
      "|u753440_20230115_0|                  1|              83393|2023-01-15 16:32:13|  2023-01-15|\n",
      "|u820171_20230115_0|                  1|             166387|2023-01-15 19:00:48|  2023-01-15|\n",
      "|u823530_20230115_0|                  1|              96688|2023-01-15 15:16:51|  2023-01-15|\n",
      "|u045962_20230115_0|                  1|              91880|2023-01-15 17:44:12|  2023-01-15|\n",
      "|u049227_20230115_0|                  1|             214162|2023-01-15 18:06:09|  2023-01-15|\n",
      "|u101625_20230115_0|                  1|             189085|2023-01-15 01:01:31|  2023-01-15|\n",
      "|u178589_20230115_0|                  1|             181083|2023-01-15 00:02:39|  2023-01-15|\n",
      "|u196122_20230115_0|                  1|             164425|2023-01-15 00:40:22|  2023-01-15|\n",
      "+------------------+-------------------+-------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sess_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goodnotes-insights-data-eng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
